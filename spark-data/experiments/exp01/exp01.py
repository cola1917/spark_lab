from pyspark.sql import SparkSession
import time

spark = (
    SparkSession.builder
    .appName("exp01_job_stage_task")
    .master("spark://spark-master:7077") 
    .getOrCreate()
)

sc = spark.sparkContext

data = sc.parallelize(range(1, 10000000), numSlices=4)

# --- ğŸ§ª Job 1: çª„ä¾èµ–å®éªŒ ---
# ä½¿ç”¨ setJobDescription è®©ä½ åœ¨ Spark UI çš„ Job é¡µé¢ç›´æ¥çœ‹åˆ°è¯´æ˜
sc.setJobDescription("Step1: Narrow Dependency (Filter + Count)")

# ç®€å•è®¡ç®—ï¼šåªæœ‰ Stage 0
count_result = data.filter(lambda x: x % 2 == 0).count()


# --- ğŸ§ª Job 2: å®½ä¾èµ–å®éªŒ ---
sc.setJobDescription("Step2: Wide Dependency (ReduceByKey + Collect)")

# é€»è¾‘ï¼šå¼ºåˆ¶è§¦å‘ Shuffle
group_result = (
    data.map(lambda x: (x % 100, 1))
    .reduceByKey(lambda a, b: a + b)  # ğŸš¨ äº§ç”Ÿ Shuffleï¼Œåˆ‡åˆ† Stage
    .filter(lambda x: x[1] > 0)
    .collect()
)

# ä¿æŒ Driver å­˜æ´»ä¸€æ®µæ—¶é—´ï¼Œæ–¹ä¾¿ä½ å» 4040 é¡µé¢æˆ– History Server æŸ¥çœ‹
time.sleep(300)

spark.stop()