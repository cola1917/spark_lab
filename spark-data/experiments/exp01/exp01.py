from pyspark.sql import SparkSession
import time

spark = (
    SparkSession.builder
    .appName("exp01_job_stage_task")
    .master("spark://spark-master:7077") 
    .getOrCreate()
)

sc = spark.sparkContext

data = sc.parallelize(range(1, 10000000), numSlices=4)

# --- ðŸ§ª Job 1: çª„ä¾èµ–å®žéªŒ ---
# ä½¿ç”¨ setJobDescription è®©ä½ åœ¨ Spark UI çš„ Job é¡µé¢ç›´æŽ¥çœ‹åˆ°è¯´æ˜Ž
sc.setJobDescription("Step1: Narrow Dependency (Filter + Count)")

# ç®€å•è®¡ç®—ï¼šåªæœ‰ Stage 0
count_result = data.filter(lambda x: x % 2 == 0).count()


# --- ðŸ§ª Job 2: å®½ä¾èµ–å®žéªŒ ---
sc.setJobDescription("Step2: Wide Dependency (ReduceByKey + Collect)")

# é€»è¾‘ï¼šå¼ºåˆ¶è§¦å‘ Shuffle
group_result = (
    data.map(lambda x: (x % 100, 1))
    .reduceByKey(lambda a, b: a + b)  # ðŸš¨ äº§ç”Ÿ Shuffleï¼Œåˆ‡åˆ† Stage
    .filter(lambda x: x[1] > 0)
    .collect()
)
spark.stop()